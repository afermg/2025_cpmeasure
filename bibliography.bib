@article{caicedoDataanalysisStrategiesImagebased2017,
  title = {Data-Analysis Strategies for Image-Based Cell Profiling},
  author = {Caicedo, Juan C and Cooper, Sam and Heigwer, Florian and Warchal, Scott and Qiu, Peng and Molnar, Csaba and Vasilevich, Aliaksei S and Barry, Joseph D and Bansal, Harmanjit Singh and Kraus, Oren and Wawer, Mathias and Paavolainen, Lassi and Herrmann, Markus D and Rohban, Mohammad and Hung, Jane and Hennig, Holger and Concannon, John and Smith, Ian and Clemons, Paul A and Singh, Shantanu and Rees, Paul and Horvath, Peter and Linington, Roger G and Carpenter, Anne E},
  year = {2017},
  month = sep,
  journal = {Nature Methods},
  volume = {14},
  number = {9},
  pages = {849--863},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.4397},
  urldate = {2025-05-13},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/KP32ZNEN/Caicedo et al. - 2017 - Data-analysis strategies for image-based cell profiling.pdf}
}

@misc{chandrasekaranJUMPCellPainting2023,
  title = {{{JUMP Cell Painting}} Dataset: Morphological Impact of 136,000 Chemical and Genetic Perturbations},
  shorttitle = {{{JUMP Cell Painting}} Dataset},
  author = {Chandrasekaran, Srinivas Niranj and Ackerman, Jeanelle and Alix, Eric and Ando, D. Michael and Arevalo, John and Bennion, Melissa and Boisseau, Nicolas and Borowa, Adriana and Boyd, Justin D. and Brino, Laurent and Byrne, Patrick J. and Ceulemans, Hugo and Ch'ng, Carolyn and Cimini, Beth A. and Clevert, Djork-Arne and Deflaux, Nicole and Doench, John G. and Dorval, Thierry and Doyonnas, Regis and Dragone, Vincenza and Engkvist, Ola and Faloon, Patrick W. and Fritchman, Briana and Fuchs, Florian and Garg, Sakshi and Gilbert, Tamara J. and Glazer, David and Gnutt, David and Goodale, Amy and Grignard, Jeremy and Guenther, Judith and Han, Yu and Hanifehlou, Zahra and Hariharan, Santosh and Hernandez, Desiree and Horman, Shane R. and Hormel, Gisela and Huntley, Michael and Icke, Ilknur and Iida, Makiyo and Jacob, Christina B. and Jaensch, Steffen and Khetan, Jawahar and {Kost-Alimova}, Maria and Krawiec, Tomasz and Kuhn, Daniel and Lardeau, Charles-Hugues and Lembke, Amanda and Lin, Francis and Little, Kevin D. and Lofstrom, Kenneth R. and Lotfi, Sofia and Logan, David J. and Luo, Yi and Madoux, Franck and Zapata, Paula A. Marin and Marion, Brittany A. and Martin, Glynn and McCarthy, Nicola Jane and Mervin, Lewis and Miller, Lisa and Mohamed, Haseeb and Monteverde, Tiziana and Mouchet, Elizabeth and Nicke, Barbara and Ogier, Arnaud and Ong, Anne-Laure and Osterland, Marc and Otrocka, Magdalena and Peeters, Pieter J. and Pilling, James and Prechtl, Stefan and Qian, Chen and Rataj, Krzysztof and Root, David E. and Sakata, Sylvie K. and Scrace, Simon and Shimizu, Hajime and Simon, David and Sommer, Peter and Spruiell, Craig and Sumia, Iffat and Swalley, Susanne E. and Terauchi, Hiroki and Thibaudeau, Amandine and Unruh, Amy and de Waeter, Jelle Van and Dyck, Michiel Van and van Staden, Carlo and Warcho{\l}, Micha{\l} and Weisbart, Erin and Weiss, Am{\'e}lie and {Wiest-Daessle}, Nicolas and Williams, Guy and Yu, Shan and Zapiec, Bolek and {\.Z}y{\l}a, Marek and Singh, Shantanu and Carpenter, Anne E.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.03.23.534023},
  publisher = {bioRxiv},
  doi = {10.1101/2023.03.23.534023},
  urldate = {2025-05-13},
  abstract = {Image-based profiling has emerged as a powerful technology for various steps in basic biological and pharmaceutical discovery, but the community has lacked a large, public reference set of data from chemical and genetic perturbations. Here we present data generated by the Joint Undertaking for Morphological Profiling (JUMP)-Cell Painting Consortium, a collaboration between 10 pharmaceutical companies, six supporting technology companies, and two non-profit partners. When completed, the dataset will contain images and profiles from the Cell Painting assay for over 116,750 unique compounds, over-expression of 12,602 genes, and knockout of 7,975 genes using CRISPR-Cas9, all in human osteosarcoma cells (U2OS). The dataset is estimated to be 115 TB in size and capturing 1.6 billion cells and their single-cell profiles. File quality control and upload is underway and will be completed over the coming months at the Cell Painting Gallery: https://registry.opendata.aws/cellpainting-gallery. A portal to visualize a subset of the data is available at https://phenaid.ardigen.com/jumpcpexplorer/.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/PCAUUL3X/Chandrasekaran et al. - 2023 - JUMP Cell Painting dataset morphological impact of 136,000 chemical and genetic perturbations.pdf}
}

@article{comoletHighlyEfficientScalable2024,
  title = {A Highly Efficient, Scalable Pipeline for Fixed Feature Extraction from Large-Scale High-Content Imaging Screens},
  author = {Comolet, Gabriel and Bose, Neeloy and Winchell, Jeff and {Duren-Lubanski}, Alyssa and Rusielewicz, Tom and Goldberg, Jordan and Horn, Grayson and Paull, Daniel and Migliori, Bianca},
  year = {2024},
  month = dec,
  journal = {iScience},
  volume = {27},
  number = {12},
  publisher = {Elsevier},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2024.111434},
  urldate = {2025-05-06},
  langid = {english},
  keywords = {Bioinformatics,Biological sciences,Medical informatics,Natural sciences},
  file = {/home/amunoz/Zotero/storage/EABWQ9RM/Comolet et al. - 2024 - A highly efficient, scalable pipeline for fixed feature extraction from large-scale high-content ima.pdf}
}

@misc{CosMxSMIMouse2025,
  title = {{{CosMx SMI Mouse Brain FFPE Dataset}}},
  year = {2025},
  journal = {NanoString},
  urldate = {2025-05-20},
  abstract = {See the power of CosMx SMI in this open-source dataset on mouse brain tissue showing over 1500 transcripts per cell with 800+ unique genes.},
  langid = {american},
  file = {/home/amunoz/Zotero/storage/DTEB5R85/cosmx-smi-mouse-brain-ffpe-dataset.html}
}

@misc{einarolafssonSpaCr2025,
  title = {{{SpaCr}}},
  author = {EinarOlafsson},
  year = {2025},
  month = apr,
  urldate = {2025-05-06},
  abstract = {spatial phenotype analysis of  CRISPR/Cas-9 screens},
  copyright = {MIT}
}

@article{ideharaExploringNileRed2025,
  title = {Exploring {{Nile Red}} Staining as an Analytical Tool for Surface-Oxidized Microplastics},
  author = {Idehara, Wakaba and Haga, Yuya and Tsujino, Hirofumi and Ikuno, Yudai and Manabe, Sota and Hokaku, Mii and Asahara, Haruyasu and Higashisaka, Kazuma and Tsutsumi, Yasuo},
  year = {2025},
  month = mar,
  journal = {Environmental Research},
  volume = {269},
  pages = {120934},
  issn = {1096-0953},
  doi = {10.1016/j.envres.2025.120934},
  abstract = {Microplastics (MPs), defined as plastic particles smaller than 5~mm, have garnered considerable attention owing to their potential biological impact on human health. These particles exhibit a range of physicochemical properties, including size, shape, and surface oxidation. Nile Red is a prominent tool for detecting microplastics, enabling staining for dynamic analyses within biological systems. However, the efficacy of Nile Red staining for surface-oxidized MPs remains unclear. Therefore, we applied Nile Red dye to stain surface-oxidized polyethylene and polyvinyl chloride and observed that both materials were effectively stained, although the fluorescence intensity varied according to different hydrophobic dynamics. Imaging analysis revealed a correlation between the fluorescence intensity score and the degree of surface oxidation, as determined using the carbonyl index calculated from attenuated total reflection-Fourier transform infrared spectroscopy data. Collectively, these findings offer novel analytical approaches for investigating environmental MPs, enhancing our understanding of their behavior and impact.},
  langid = {english},
  pmid = {39862951},
  keywords = {Carbonyl index,Environmental monitoring,Environmental Monitoring,Fluorescence imaging,Fluorescent Dyes,Microplastic analysis,Microplastics,Oxazines,Oxidation-Reduction,Polyethylene,Polyvinyl Chloride,Spectroscopy Fourier Transform Infrared,Staining and Labeling,Surface oxidation,Surface Properties,Water Pollutants Chemical}
}

@inproceedings{kalinin3DCellNuclear2018,
  title = {{{3D Cell Nuclear Morphology}}: {{Microscopy Imaging Dataset}} and {{Voxel-Based Morphometry Classification Results}}},
  shorttitle = {{{3D Cell Nuclear Morphology}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Kalinin, Alexandr A. and {Allyn-Feuer}, Ari and Ade, Alex and Fon, Gordon-Victor and Meixner, Walter and Dilworth, David and {de Wet}, Jeffrey R. and Higgins, Gerald A. and Zheng, Gen and Creekmore, Amy and Wiley, John W. and Verdone, James E. and Veltri, Robert W. and Pienta, Kenneth J. and Coffey, Donald S. and Athey, Brian D. and Dinov, Ivo D.},
  year = {2018},
  pages = {2272--2280},
  urldate = {2025-05-13},
  file = {/home/amunoz/Zotero/storage/5K785794/Kalinin et al. - 2018 - 3D Cell Nuclear Morphology Microscopy Imaging Dataset and Voxel-Based Morphometry Classification Re.pdf}
}

@inproceedings{lamNumbaLLVMbasedPython2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  year = {2015},
  month = nov,
  series = {{{LLVM}} '15},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2833157.2833162},
  urldate = {2025-05-13},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  file = {/home/amunoz/Zotero/storage/YD35Y5F6/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@article{marconatoSpatialDataOpenUniversal2025,
  title = {{{SpatialData}}: An Open and Universal Data Framework for Spatial Omics},
  shorttitle = {{{SpatialData}}},
  author = {Marconato, Luca and Palla, Giovanni and Yamauchi, Kevin A. and Virshup, Isaac and Heidari, Elyas and Treis, Tim and Vierdag, Wouter-Michiel and Toth, Marcella and Stockhaus, Sonja and Shrestha, Rahul B. and Rombaut, Benjamin and Pollaris, Lotte and Lehner, Laurens and V{\"o}hringer, Harald and Kats, Ilia and Saeys, Yvan and Saka, Sinem K. and Huber, Wolfgang and Gerstung, Moritz and Moore, Josh and Theis, Fabian J. and Stegle, Oliver},
  year = {2025},
  month = jan,
  journal = {Nature Methods},
  volume = {22},
  number = {1},
  pages = {58--62},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02212-x},
  urldate = {2025-05-20},
  abstract = {Spatially resolved omics technologies are transforming our understanding of biological tissues. However, the handling of uni- and multimodal spatial omics datasets remains a challenge owing to large data volumes, heterogeneity of data types and the lack of flexible, spatially aware data structures. Here we introduce SpatialData, a framework that establishes a unified and extensible multiplatform file-format, lazy representation of larger-than-memory data, transformations and alignment to common coordinate systems. SpatialData facilitates spatial annotations and cross-modal aggregation and analysis, the utility of which is illustrated in the context of multiple vignettes, including integrative analysis on a multimodal Xenium and Visium breast cancer study.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational platforms and environments,Data integration,Molecular imaging,Software},
  file = {/home/amunoz/Zotero/storage/KU5VUM2H/Marconato et al. - 2025 - SpatialData an open and universal data framework for spatial omics.pdf}
}

@article{mcquinCellProfiler30Nextgeneration2018,
  title = {{{CellProfiler}} 3.0: {{Next-generation}} Image Processing for Biology},
  shorttitle = {{{CellProfiler}} 3.0},
  author = {McQuin, Claire and Goodman, Allen and Chernyshev, Vasiliy and Kamentsky, Lee and Cimini, Beth A. and Karhohs, Kyle W. and Doan, Minh and Ding, Liya and Rafelski, Susanne M. and Thirstrup, Derek and Wiegraebe, Winfried and Singh, Shantanu and Becker, Tim and Caicedo, Juan C. and Carpenter, Anne E.},
  year = {2018},
  month = jul,
  journal = {PLOS Biology},
  volume = {16},
  number = {7},
  pages = {e2005970},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2005970},
  urldate = {2025-05-08},
  abstract = {CellProfiler has enabled the scientific research community to create flexible, modular image analysis pipelines since its release in 2005. Here, we describe CellProfiler 3.0, a new version of the software supporting both whole-volume and plane-wise analysis of three-dimensional (3D) image stacks, increasingly common in biomedical research. CellProfiler's infrastructure is greatly improved, and we provide a protocol for cloud-based, large-scale image processing. New plugins enable running pretrained deep learning models on images. Designed by and for biologists, CellProfiler equips researchers with powerful computational tools via a well-documented user interface, empowering biologists in all fields to create quantitative, reproducible image analysis workflows.},
  langid = {english},
  keywords = {Biologists,Blastocysts,Cell staining,Computer software,Deep learning,Image analysis,Image processing,Open source software},
  file = {/home/amunoz/Zotero/storage/ZHFPXNFQ/McQuin et al. - 2018 - CellProfiler 3.0 Next-generation image processing for biology.pdf}
}

@article{moenDeepLearningCellular2019,
  title = {Deep Learning for Cellular Image Analysis},
  author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
  year = {2019},
  month = dec,
  journal = {Nature Methods},
  volume = {16},
  number = {12},
  pages = {1233--1246},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0403-1},
  urldate = {2025-05-08},
  abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
  copyright = {2019 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Image processing,Software},
  file = {/home/amunoz/Zotero/storage/ZEI8L6EP/Moen et al. - 2019 - Deep learning for cellular image analysis.pdf}
}

@article{pachitariuCellpose20How2022,
  title = {Cellpose 2.0: How to Train Your Own Model},
  shorttitle = {Cellpose 2.0},
  author = {Pachitariu, Marius and Stringer, Carsen},
  year = {2022},
  month = dec,
  journal = {Nature Methods},
  volume = {19},
  number = {12},
  pages = {1634--1641},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-022-01663-4},
  urldate = {2025-05-06},
  abstract = {Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500--1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100--200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational platforms and environments,Image processing},
  file = {/home/amunoz/Zotero/storage/B6TZE2BA/Pachitariu and Stringer - 2022 - Cellpose 2.0 how to train your own model.pdf}
}

@article{pallaSquidpyScalableFramework2022,
  title = {Squidpy: A Scalable Framework for Spatial Omics Analysis},
  shorttitle = {Squidpy},
  author = {Palla, Giovanni and Spitzer, Hannah and Klein, Michal and Fischer, David and Schaar, Anna Christina and Kuemmerle, Louis Benedikt and Rybakov, Sergei and Ibarra, Ignacio L. and Holmberg, Olle and Virshup, Isaac and Lotfollahi, Mohammad and Richter, Sabrina and Theis, Fabian J.},
  year = {2022},
  month = feb,
  journal = {Nature Methods},
  volume = {19},
  number = {2},
  pages = {171--178},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01358-2},
  urldate = {2025-05-20},
  abstract = {Spatial omics data are advancing the study of tissue organization and cellular communication at an unprecedented scale. Flexible tools are required to store, integrate and visualize the large diversity of spatial omics data. Here, we present Squidpy, a Python framework that brings together tools from omics and image analysis to enable scalable description of spatial molecular data, such as transcriptome or multivariate proteins. Squidpy provides efficient infrastructure and numerous analysis methods that allow to efficiently store, manipulate and interactively visualize spatial omics data. Squidpy is extensible and can be interfaced with a variety of already existing libraries for the scalable analysis of spatial omics data.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Data integration,Imaging,Software,Transcriptomics},
  file = {/home/amunoz/Zotero/storage/WEFJJYTG/Palla et al. - 2022 - Squidpy a scalable framework for spatial omics analysis.pdf}
}

@article{sealDecadeSystematicReview2024,
  title = {A {{Decade}} in a {{Systematic Review}}: {{The Evolution}} and {{Impact}} of {{Cell Painting}}},
  shorttitle = {A {{Decade}} in a {{Systematic Review}}},
  author = {Seal, Srijit and Trapotsi, Maria-Anna and Spjuth, Ola and Singh, Shantanu and {Carreras-Puigvert}, Jordi and Greene, Nigel and Bender, Andreas and Carpenter, Anne E.},
  year = {2024},
  month = may,
  journal = {bioRxiv},
  pages = {2024.05.04.592531},
  issn = {2692-8205},
  doi = {10.1101/2024.05.04.592531},
  urldate = {2025-05-13},
  abstract = {High-content image-based assays have fueled significant discoveries in the life sciences in the past decade (2013--2023), including novel insights into disease etiology, mechanism of action, new therapeutics, and toxicology predictions. Here, we systematically review the substantial methodological advancements and applications of Cell Painting. Advancements include improvements in the Cell Painting protocol, assay adaptations for different types of perturbations and applications, and improved methodologies for feature extraction, quality control, and batch effect correction. Moreover, machine learning methods recently surpassed classical approaches in their ability to extract biologically useful information from Cell Painting images. Cell Painting data have been used alone or in combination with other - omics data to decipher the mechanism of action of a compound, its toxicity profile, and many other biological effects. Overall, key methodological advances have expanded Cell Painting's ability to capture cellular responses to various perturbations. Future advances will likely lie in advancing computational and experimental techniques, developing new publicly available datasets, and integrating them with other high-content data types.},
  pmcid = {PMC11100607},
  pmid = {38766203},
  file = {/home/amunoz/Zotero/storage/4IJP67CI/Seal et al. - 2024 - A Decade in a Systematic Review The Evolution and Impact of Cell Painting.pdf}
}

@article{serranoReproducibleImagebasedProfiling2025,
  title = {Reproducible Image-Based Profiling with {{Pycytominer}}},
  author = {Serrano, Erik and Chandrasekaran, Srinivas Niranj and Bunten, Dave and Brewer, Kenneth I. and Tomkinson, Jenna and Kern, Roshan and Bornholdt, Michael and Fleming, Stephen J. and Pei, Ruifan and Arevalo, John and Tsang, Hillary and Rubinetti, Vincent and {Tromans-Coia}, Callum and Becker, Tim and Weisbart, Erin and Bunne, Charlotte and Kalinin, Alexandr A. and Senft, Rebecca and Taylor, Stephen J. and Jamali, Nasim and Adeboye, Adeniyi and Abbasi, Hamdah Shafqat and Goodman, Allen and Caicedo, Juan C. and Carpenter, Anne E. and Cimini, Beth A. and Singh, Shantanu and Way, Gregory P.},
  year = {2025},
  month = apr,
  journal = {Nature Methods},
  volume = {22},
  number = {4},
  pages = {677--680},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-025-02611-8},
  urldate = {2025-05-13},
  abstract = {Advances in high-throughput microscopy have enabled the rapid acquisition of large numbers of high-content microscopy images. Next, whether by deep learning or classical algorithms, image analysis pipelines commonly produce single-cell features. To process these single cells for downstream applications, we present Pycytominer, a user-friendly, open-source Python package that implements the bioinformatics steps key to image-based profiling. We demonstrate Pycytominer's usefulness in a machine-learning project to predict nuisance compounds that cause undesirable cell injuries.},
  copyright = {2025 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {High-throughput screening,Machine learning,Software},
  file = {/home/amunoz/Zotero/storage/59E67ZGT/Serrano et al. - 2025 - Reproducible image-based profiling with Pycytominer.pdf}
}

@article{stirlingCellProfiler4Improvements2021,
  title = {{{CellProfiler}} 4: Improvements in Speed, Utility and Usability},
  shorttitle = {{{CellProfiler}} 4},
  author = {Stirling, David R. and {Swain-Bowden}, Madison J. and Lucas, Alice M. and Carpenter, Anne E. and Cimini, Beth A. and Goodman, Allen},
  year = {2021},
  month = sep,
  journal = {BMC Bioinformatics},
  volume = {22},
  number = {1},
  pages = {433},
  issn = {1471-2105},
  doi = {10.1186/s12859-021-04344-9},
  urldate = {2025-05-05},
  abstract = {Imaging data contains a substantial amount of information which can be difficult to evaluate by eye. With the expansion of high throughput microscopy methodologies producing increasingly large datasets, automated and objective analysis of the resulting images is essential to effectively extract biological information from this data. CellProfiler is a free, open source image analysis program which enables researchers to generate modular pipelines with which to process microscopy images into interpretable measurements.},
  langid = {english},
  keywords = {Bioimaging,Image analysis,Image quantitation,Image segmentation,Microscopy},
  file = {/home/amunoz/Zotero/storage/8D496EM6/Stirling et al. - 2021 - CellProfiler 4 improvements in speed, utility and usability.pdf}
}

@inproceedings{sundararajanManyShapleyValues2020,
  title = {The {{Many Shapley Values}} for {{Model Explanation}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Sundararajan, Mukund and Najmi, Amir},
  year = {2020},
  month = nov,
  pages = {9269--9278},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-20},
  abstract = {The Shapley value has become the basis for several methods that attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing the uniqueness result from~{\textbackslash}cite\{Shapley53\}, which shows that it is the only method that satisfies certain good properties ({\textbackslash}emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized for model explanation. These differ in how they reference the model, the training data, and the explanation context. Hence they differ in output, rendering the uniqueness result inapplicable. Furthermore, the techniques that rely on they training data produce non-intuitive attributions, for instance unused features can still receive attribution. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution. We discuss a technique called Baseline Shapley (BShap), provide a proper uniqueness result for it, and contrast it with two other techniques from prior literature, Integrated Gradients~{\textbackslash}cite\{STY17\} and Conditional Expectation Shapley~{\textbackslash}cite\{Lundberg2017AUA\}.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/XY2MJGJD/Sundararajan and Najmi - 2020 - The Many Shapley Values for Model Explanation.pdf}
}

@article{tangMorphologicalProfilingDrug2024,
  title = {Morphological Profiling for Drug Discovery in the Era of Deep Learning},
  author = {Tang, Qiaosi and Ratnayake, Ranjala and Seabra, Gustavo and Jiang, Zhe and Fang, Ruogu and Cui, Lina and Ding, Yousong and Kahveci, Tamer and Bian, Jiang and Li, Chenglong and Luesch, Hendrik and Li, Yanjun},
  year = {2024},
  month = jul,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {4},
  pages = {bbae284},
  issn = {1477-4054},
  doi = {10.1093/bib/bbae284},
  urldate = {2025-05-08},
  abstract = {Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high throughput. These efforts have facilitated understanding of compound mechanism of action, drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering-- and deep learning--based approaches, and introduce publicly available benchmark datasets. We place a particular emphasis on the application of deep learning in this pipeline, covering cell segmentation, image representation learning, and multimodal learning. Additionally, we illuminate the application of morphological profiling in phenotypic drug discovery and highlight potential challenges and opportunities in this field.},
  file = {/home/amunoz/Zotero/storage/YBYS8LNL/Tang et al. - 2024 - Morphological profiling for drug discovery in the era of deep learning.pdf;/home/amunoz/Zotero/storage/W5LLGRM8/7693952.html}
}

@article{waltScikitimageImageProcessing2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  shorttitle = {Scikit-Image},
  author = {van der Walt, St{\'e}fan and Sch{\"o}nberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  year = {2014},
  month = jun,
  journal = {PeerJ},
  volume = {2},
  pages = {e453},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.453},
  urldate = {2025-05-08},
  abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/FT4SJ6TM/Walt et al. - 2014 - scikit-image image processing in Python.pdf}
}

@article{wolfSCANPYLargescaleSinglecell2018,
  title = {{{SCANPY}}: Large-Scale Single-Cell Gene Expression Data Analysis},
  shorttitle = {{{SCANPY}}},
  author = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
  year = {2018},
  month = feb,
  journal = {Genome Biology},
  volume = {19},
  number = {1},
  pages = {15},
  issn = {1474-760X},
  doi = {10.1186/s13059-017-1382-0},
  urldate = {2025-05-20},
  abstract = {Scanpy is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with Scanpy, we present AnnData, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).},
  langid = {english},
  keywords = {Bioinformatics,Clustering,Differential expression testing,Gene Expression Analysis,Gene expression profiling,Genome-wide analysis of gene expression,Graph analysis,High-throughput Cell Screening,Machine learning,Microarray analysis,Pseudotemporal ordering,Scalability,Single-cell transcriptomics,Trajectory inference,Transcriptomics,Visualization},
  file = {/home/amunoz/Zotero/storage/9Z7V9KWX/Wolf et al. - 2018 - SCANPY large-scale single-cell gene expression data analysis.pdf}
}
