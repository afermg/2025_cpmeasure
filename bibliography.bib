@misc{adjavonQuantitativeAttributionsCounterfactuals2024,
  title = {Quantitative {{Attributions}} with {{Counterfactuals}}},
  author = {Adjavon, Diane-Yayra and Eckstein, Nils and Bates, Alexander S. and Jefferis, Gregory S.X.E. and Funke, Jan},
  year = {2024},
  month = dec,
  publisher = {Bioinformatics},
  doi = {10.1101/2024.11.26.625505},
  urldate = {2025-05-21},
  abstract = {We address the problem of explaining the decision process of deep neural network classifiers on images, which is of particular importance in biomedical datasets where class-relevant differences are not always obvious to a human observer. Our proposed solution, termed quantitative attribution with counterfactuals (QuAC), generates visual explanations that highlight class-relevant differences by attributing the classifier decision to changes of visual features in small parts of an image. To that end, we train a separate network to generate counterfactual images (i.e., to translate images between different classes). We then find the most important differences using novel discriminative attribution methods. Crucially, QuAC allows scoring of the attribution and thus provides a measure to quantify and compare the fidelity of a visual explanation. We demonstrate the suitability and limitations of QuAC on two datasets: (1) a synthetic dataset with known class differences, representing different levels of protein aggregation in cells and (2) an electron microscopy dataset of D. melanogaster synapses with different neurotransmitters, where QuAC reveals so far unknown visual differences. We further discuss how QuAC can be used to interrogate mispredictions to shed light on unexpected inter-class similarities and intra-class differences.},
  archiveprefix = {Bioinformatics},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/X32YMZHI/Adjavon et al. - 2024 - Quantitative Attributions with Counterfactuals.pdf}
}

@misc{azadFoundationalModelsMedical2023,
  title = {Foundational {{Models}} in {{Medical Imaging}}: {{A Comprehensive Survey}} and {{Future Vision}}},
  shorttitle = {Foundational {{Models}} in {{Medical Imaging}}},
  author = {Azad, Bobby and Azad, Reza and Eskandari, Sania and Bozorgpour, Afshin and Kazerouni, Amirhossein and Rekik, Islem and Merhof, Dorit},
  year = {2023},
  month = oct,
  number = {arXiv:2310.18689},
  eprint = {2310.18689},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.18689},
  urldate = {2025-05-21},
  abstract = {Foundation models, large-scale, pre-trained deep-learning models adapted to a wide range of downstream tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm shift with the rise of these models. Trained on large-scale dataset to bridge the gap between different modalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities at test time. The predictions of these models can be adjusted for new tasks by augmenting the model input with task-specific hints called prompts without requiring extensive labeled data and retraining. Capitalizing on the advances in computer vision, medical imaging has also marked a growing interest in these models. To assist researchers in navigating this direction, this survey intends to provide a comprehensive overview of foundation models in the domain of medical imaging. Specifically, we initiate our exploration by providing an exposition of the fundamental concepts forming the basis of foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the medical domain, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models. Furthermore, we emphasize the practical use case of some selected approaches and then discuss the opportunities, applications, and future directions of these large-scale pre-trained models, for analyzing medical images. In the same vein, we address the prevailing challenges and research pathways associated with foundational models in medical imaging. These encompass the areas of interpretability, data management, computational requirements, and the nuanced issue of contextual comprehension.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/amunoz/Zotero/storage/RMSJS7TK/Azad et al. - 2023 - Foundational Models in Medical Imaging A Comprehensive Survey and Future Vision.pdf;/home/amunoz/Zotero/storage/74RY9Y9N/2310.html}
}

@article{caicedoCellPaintingPredicts2022,
  title = {Cell {{Painting}} Predicts Impact of Lung Cancer Variants},
  author = {Caicedo, Juan C. and Arevalo, John and Piccioni, Federica and Bray, Mark-Anthony and Hartland, Cathy L. and Wu, Xiaoyun and Brooks, Angela N. and Berger, Alice H. and Boehm, Jesse S. and Carpenter, Anne E. and Singh, Shantanu},
  editor = {{Lippincott-Schwartz}, Jennifer},
  year = {2022},
  month = may,
  journal = {Molecular Biology of the Cell},
  volume = {33},
  number = {6},
  pages = {ar49},
  issn = {1059-1524, 1939-4586},
  doi = {10.1091/mbc.E21-11-0538},
  urldate = {2025-05-21},
  abstract = {Clinical sequencing uncovers many variants of unknown significance, especially in cancer, leaving doctors stuck. We show that cell morphology-specifically, images of cells stained for various organelles through the Cell Painting assay-contains sufficient information to determine the impact of many variants on diverse genes and functions.           ,              Most variants in most genes across most organisms have an unknown impact on the function of the corresponding gene. This gap in knowledge is especially acute in cancer, where clinical sequencing of tumors now routinely reveals patient-specific variants whose functional impact on the corresponding genes is unknown, impeding clinical utility. Transcriptional profiling was able to systematically distinguish these variants of unknown significance as impactful vs. neutral in an approach called expression-based variant-impact phenotyping. We profiled a set of lung adenocarcinoma-associated somatic variants using Cell Painting, a morphological profiling assay that captures features of cells based on microscopy using six stains of cell and organelle components. Using deep-learning-extracted features from each cell's image, we found that cell morphological profiling (cmVIP) can predict variants' functional impact and, particularly at the single-cell level, reveals biological insights into variants that can be explored at our public online portal. Given its low cost, convenient implementation, and single-cell resolution, cmVIP profiling therefore seems promising as an avenue for using non--gene specific assays to systematically assess the impact of variants, including disease-associated alleles, on gene function.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/USU8P38H/Caicedo et al. - 2022 - Cell Painting predicts impact of lung cancer variants.pdf}
}

@article{caicedoDataanalysisStrategiesImagebased2017,
  title = {Data-Analysis Strategies for Image-Based Cell Profiling},
  author = {Caicedo, Juan C and Cooper, Sam and Heigwer, Florian and Warchal, Scott and Qiu, Peng and Molnar, Csaba and Vasilevich, Aliaksei S and Barry, Joseph D and Bansal, Harmanjit Singh and Kraus, Oren and Wawer, Mathias and Paavolainen, Lassi and Herrmann, Markus D and Rohban, Mohammad and Hung, Jane and Hennig, Holger and Concannon, John and Smith, Ian and Clemons, Paul A and Singh, Shantanu and Rees, Paul and Horvath, Peter and Linington, Roger G and Carpenter, Anne E},
  year = {2017},
  month = sep,
  journal = {Nature Methods},
  volume = {14},
  number = {9},
  pages = {849--863},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.4397},
  urldate = {2025-05-13},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/KP32ZNEN/Caicedo et al. - 2017 - Data-analysis strategies for image-based cell profiling.pdf}
}

@misc{chandrasekaranJUMPCellPainting2023,
  title = {{{JUMP Cell Painting}} Dataset: Morphological Impact of 136,000 Chemical and Genetic Perturbations},
  shorttitle = {{{JUMP Cell Painting}} Dataset},
  author = {Chandrasekaran, Srinivas Niranj and Ackerman, Jeanelle and Alix, Eric and Ando, D. Michael and Arevalo, John and Bennion, Melissa and Boisseau, Nicolas and Borowa, Adriana and Boyd, Justin D. and Brino, Laurent and Byrne, Patrick J. and Ceulemans, Hugo and Ch'ng, Carolyn and Cimini, Beth A. and Clevert, Djork-Arne and Deflaux, Nicole and Doench, John G. and Dorval, Thierry and Doyonnas, Regis and Dragone, Vincenza and Engkvist, Ola and Faloon, Patrick W. and Fritchman, Briana and Fuchs, Florian and Garg, Sakshi and Gilbert, Tamara J. and Glazer, David and Gnutt, David and Goodale, Amy and Grignard, Jeremy and Guenther, Judith and Han, Yu and Hanifehlou, Zahra and Hariharan, Santosh and Hernandez, Desiree and Horman, Shane R. and Hormel, Gisela and Huntley, Michael and Icke, Ilknur and Iida, Makiyo and Jacob, Christina B. and Jaensch, Steffen and Khetan, Jawahar and {Kost-Alimova}, Maria and Krawiec, Tomasz and Kuhn, Daniel and Lardeau, Charles-Hugues and Lembke, Amanda and Lin, Francis and Little, Kevin D. and Lofstrom, Kenneth R. and Lotfi, Sofia and Logan, David J. and Luo, Yi and Madoux, Franck and Zapata, Paula A. Marin and Marion, Brittany A. and Martin, Glynn and McCarthy, Nicola Jane and Mervin, Lewis and Miller, Lisa and Mohamed, Haseeb and Monteverde, Tiziana and Mouchet, Elizabeth and Nicke, Barbara and Ogier, Arnaud and Ong, Anne-Laure and Osterland, Marc and Otrocka, Magdalena and Peeters, Pieter J. and Pilling, James and Prechtl, Stefan and Qian, Chen and Rataj, Krzysztof and Root, David E. and Sakata, Sylvie K. and Scrace, Simon and Shimizu, Hajime and Simon, David and Sommer, Peter and Spruiell, Craig and Sumia, Iffat and Swalley, Susanne E. and Terauchi, Hiroki and Thibaudeau, Amandine and Unruh, Amy and de Waeter, Jelle Van and Dyck, Michiel Van and van Staden, Carlo and Warcho{\l}, Micha{\l} and Weisbart, Erin and Weiss, Am{\'e}lie and {Wiest-Daessle}, Nicolas and Williams, Guy and Yu, Shan and Zapiec, Bolek and {\.Z}y{\l}a, Marek and Singh, Shantanu and Carpenter, Anne E.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.03.23.534023},
  publisher = {bioRxiv},
  doi = {10.1101/2023.03.23.534023},
  urldate = {2025-05-13},
  abstract = {Image-based profiling has emerged as a powerful technology for various steps in basic biological and pharmaceutical discovery, but the community has lacked a large, public reference set of data from chemical and genetic perturbations. Here we present data generated by the Joint Undertaking for Morphological Profiling (JUMP)-Cell Painting Consortium, a collaboration between 10 pharmaceutical companies, six supporting technology companies, and two non-profit partners. When completed, the dataset will contain images and profiles from the Cell Painting assay for over 116,750 unique compounds, over-expression of 12,602 genes, and knockout of 7,975 genes using CRISPR-Cas9, all in human osteosarcoma cells (U2OS). The dataset is estimated to be 115 TB in size and capturing 1.6 billion cells and their single-cell profiles. File quality control and upload is underway and will be completed over the coming months at the Cell Painting Gallery: https://registry.opendata.aws/cellpainting-gallery. A portal to visualize a subset of the data is available at https://phenaid.ardigen.com/jumpcpexplorer/.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/PCAUUL3X/Chandrasekaran et al. - 2023 - JUMP Cell Painting dataset morphological impact of 136,000 chemical and genetic perturbations.pdf}
}

@ARTICLE{Korsunsky2019-Harmony,
  title     = "Fast, sensitive and accurate integration of single-cell data with
               Harmony",
  author    = "Korsunsky, Ilya and Millard, Nghia and Fan, Jean and Slowikowski,
               Kamil and Zhang, Fan and Wei, Kevin and Baglaenko, Yuriy and
               Brenner, Michael and Loh, Po-Ru and Raychaudhuri, Soumya",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  16,
  number    =  12,
  pages     = "1289--1296",
  abstract  = "The emerging diversity of single-cell RNA-seq datasets allows for
               the full transcriptional characterization of cell types across a
               wide variety of biological and clinical conditions. However, it
               is challenging to analyze them together, particularly when
               datasets are assayed with different technologies, because
               biological and technical differences are interspersed. We present
               Harmony (https://github.com/immunogenomics/harmony), an algorithm
               that projects cells into a shared embedding in which cells group
               by cell type rather than dataset-specific conditions. Harmony
               simultaneously accounts for multiple experimental and biological
               factors. In six analyses, we demonstrate the superior performance
               of Harmony to previously published algorithms while requiring
               fewer computational resources. Harmony enables the integration of
               ~106 cells on a personal computer. We apply Harmony to peripheral
               blood mononuclear cells from datasets with large experimental
               differences, five studies of pancreatic islet cells, mouse
               embryogenesis datasets and the integration of scRNA-seq with
               spatial transcriptomics data.",
  month     =  dec,
  year      =  2019,
  language  = "en"
}


@article{chowPredictingDrugPolypharmacology2022,
  title = {Predicting Drug Polypharmacology from Cell Morphology Readouts Using Variational Autoencoder Latent Space Arithmetic},
  author = {Chow, Yuen Ler and Singh, Shantanu and Carpenter, Anne E. and Way, Gregory P.},
  editor = {Haugh, Jason M.},
  year = {2022},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {2},
  pages = {e1009888},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009888},
  urldate = {2025-05-21},
  abstract = {A variational autoencoder (VAE) is a machine learning algorithm, useful for generating a compressed and interpretable latent space. These representations have been generated from various biomedical data types and can be used to produce realistic-looking simulated data. However, standard vanilla VAEs suffer from entangled and uninformative latent spaces, which can be mitigated using other types of VAEs such as {$\beta$}-VAE and MMD-VAE. In this project, we evaluated the ability of VAEs to learn cell morphology characteristics derived from cell images. We trained and evaluated these three VAE variants---Vanilla VAE, {$\beta$}-VAE, and MMD-VAE---on cell morphology readouts and explored the generative capacity of each model to predict compound polypharmacology (the interactions of a drug with more than one target) using an approach called latent space arithmetic (LSA). To test the generalizability of the strategy, we also trained these VAEs using gene expression data of the same compound perturbations and found that gene expression provides complementary information. We found that the {$\beta$}-VAE and MMD-VAE disentangle morphology signals and reveal a more interpretable latent space. We reliably simulated morphology and gene expression readouts from certain compounds thereby predicting cell states perturbed with compounds of known polypharmacology. Inferring cell state for specific drug mechanisms could aid researchers in developing and identifying targeted therapeutics and categorizing off-target effects in the future.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/YKPAANML/Chow et al. - 2022 - Predicting drug polypharmacology from cell morphology readouts using variational autoencoder latent.pdf}
}

@article{comoletHighlyEfficientScalable2024,
  title = {A Highly Efficient, Scalable Pipeline for Fixed Feature Extraction from Large-Scale High-Content Imaging Screens},
  author = {Comolet, Gabriel and Bose, Neeloy and Winchell, Jeff and {Duren-Lubanski}, Alyssa and Rusielewicz, Tom and Goldberg, Jordan and Horn, Grayson and Paull, Daniel and Migliori, Bianca},
  year = {2024},
  month = dec,
  journal = {iScience},
  volume = {27},
  number = {12},
  publisher = {Elsevier},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2024.111434},
  urldate = {2025-05-06},
  langid = {english},
  keywords = {Bioinformatics,Biological sciences,Medical informatics,Natural sciences},
  file = {/home/amunoz/Zotero/storage/EABWQ9RM/Comolet et al. - 2024 - A highly efficient, scalable pipeline for fixed feature extraction from large-scale high-content ima.pdf}
}

@misc{CosMxSMIMouse2025,
  title = {{{CosMx SMI Mouse Brain FFPE Dataset}}},
  year = {2025},
  journal = {NanoString},
  urldate = {2025-05-20},
  abstract = {See the power of CosMx SMI in this open-source dataset on mouse brain tissue showing over 1500 transcripts per cell with 800+ unique genes.},
  langid = {american},
  file = {/home/amunoz/Zotero/storage/DTEB5R85/cosmx-smi-mouse-brain-ffpe-dataset.html}
}

@misc{einarolafssonSpaCr2025,
  title = {{{Olafsson EB, et al. SpaCr: Spatial phenotype analysis of CRISPR-Cas9 screens. Manuscript in preparation}}},
  author = {Olafsson, Einar},
  year = {2025},
  month = apr,
  urldate = {2025-05-06},
  abstract = {spatial phenotype analysis of  CRISPR/Cas-9 screens},
  copyright = {MIT}
}

@misc{fayRxRx3PhenomicsMap2023,
  title = {{{RxRx3}}: {{Phenomics Map}} of {{Biology}}},
  shorttitle = {{{RxRx3}}},
  author = {Fay, Marta M. and Kraus, Oren and Victors, Mason and Arumugam, Lakshmanan and Vuggumudi, Kamal and Urbanik, John and Hansen, Kyle and Celik, Safiye and Cernek, Nico and Jagannathan, Ganesh and Christensen, Jordan and Earnshaw, Berton A. and Haque, Imran S. and Mabey, Ben},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.07.527350},
  publisher = {bioRxiv},
  doi = {10.1101/2023.02.07.527350},
  urldate = {2025-05-21},
  abstract = {The combination of modern genetic perturbation techniques with high content screening has enabled genome-scale cell microscopy experiments that can be leveraged to construct maps of biology. These are built by processing microscopy images to produce readouts in unified and relatable representation space to capture known biological relationships and discover new ones. To further enable the scientific community to develop methods and insights from map-scale data, here we release RxRx3, the first ever public high-content screening dataset combining genome-scale CRISPR knockouts with multiple-concentration screening of small molecules (a set of FDA approved and commercially available bioactive compounds). The dataset contains 6-channel fluorescent microscopy images and associated deep learning embeddings from over 2.2 million wells that span 17,063 CRISPR knockouts and 1,674 compounds at 8 doses each. RxRx3 is one of the largest collections of cellular screening data, and as far as we know, the largest generated consistently via a common experimental protocol within a single laboratory. Our goal in releasing RxRx3 is to demonstrate the benefits of generating consistent data, enable the development of the machine learning methods on this scale of data and to foster research, methods development, and collaboration. For more information about RxRx3 please visit RxRx.ai/rxrx3},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/KYICU2AT/Fay et al. - 2023 - RxRx3 Phenomics Map of Biology.pdf}
}

@article{garcia-fossaInterpretingImagebasedProfiles2023,
  title = {Interpreting {{Image-based Profiles}} Using {{Similarity Clustering}} and {{Single-Cell Visualization}}},
  author = {{Garcia-Fossa}, Fernanda and Cruz, Mario Costa and Haghighi, Marzieh and {de Jesus}, Marcelo Bispo and Singh, Shantanu and Carpenter, Anne E. and Cimini, Beth A.},
  year = {2023},
  journal = {Current Protocols},
  volume = {3},
  number = {3},
  pages = {e713},
  issn = {2691-1299},
  doi = {10.1002/cpz1.713},
  urldate = {2025-05-23},
  abstract = {Image-based profiling quantitatively assesses the effects of perturbations on cells by capturing a breadth of changes via microscopy. Here, we provide two complementary protocols to help explore and interpret data from image-based profiling experiments. In the first protocol, we examine the similarity among perturbed cell samples using data from compounds that cluster by their mechanisms of action. The protocol includes steps to examine feature-driving differences between samples and to visualize correlations between features and treatments to create interpretable heatmaps using the open-source web tool Morpheus. In the second protocol, we show how to interactively explore images together with the numerical data, and we provide scripts to create visualizations of representative single cells and image sites to understand how changes in features are reflected in the images. Together, these two tutorials help researchers interpret image-based data to speed up research. {\copyright} 2023 The Authors. Current Protocols published by Wiley Periodicals LLC. Basic Protocol 1: Exploratory analysis of profile similarities and driving features Basic Protocol 2: Image and single-cell visualization following profile interpretation},
  langid = {english},
  keywords = {high-dimensional data,image-based profiling,Morpheus,morphological analysis,profiling,single-cell visualization},
  file = {/home/amunoz/Zotero/storage/TGG7EEN9/Garcia-Fossa et al. - 2023 - Interpreting Image-based Profiles using Similarity Clustering and Single-Cell Visualization.pdf;/home/amunoz/Zotero/storage/7WR6S4IX/cpz1.html}
}

@article{ideharaExploringNileRed2025,
  title = {Exploring {{Nile Red}} Staining as an Analytical Tool for Surface-Oxidized Microplastics},
  author = {Idehara, Wakaba and Haga, Yuya and Tsujino, Hirofumi and Ikuno, Yudai and Manabe, Sota and Hokaku, Mii and Asahara, Haruyasu and Higashisaka, Kazuma and Tsutsumi, Yasuo},
  year = {2025},
  month = mar,
  journal = {Environmental Research},
  volume = {269},
  pages = {120934},
  issn = {1096-0953},
  doi = {10.1016/j.envres.2025.120934},
  abstract = {Microplastics (MPs), defined as plastic particles smaller than 5~mm, have garnered considerable attention owing to their potential biological impact on human health. These particles exhibit a range of physicochemical properties, including size, shape, and surface oxidation. Nile Red is a prominent tool for detecting microplastics, enabling staining for dynamic analyses within biological systems. However, the efficacy of Nile Red staining for surface-oxidized MPs remains unclear. Therefore, we applied Nile Red dye to stain surface-oxidized polyethylene and polyvinyl chloride and observed that both materials were effectively stained, although the fluorescence intensity varied according to different hydrophobic dynamics. Imaging analysis revealed a correlation between the fluorescence intensity score and the degree of surface oxidation, as determined using the carbonyl index calculated from attenuated total reflection-Fourier transform infrared spectroscopy data. Collectively, these findings offer novel analytical approaches for investigating environmental MPs, enhancing our understanding of their behavior and impact.},
  langid = {english},
  pmid = {39862951},
  keywords = {Carbonyl index,Environmental monitoring,Environmental Monitoring,Fluorescence imaging,Fluorescent Dyes,Microplastic analysis,Microplastics,Oxazines,Oxidation-Reduction,Polyethylene,Polyvinyl Chloride,Spectroscopy Fourier Transform Infrared,Staining and Labeling,Surface oxidation,Surface Properties,Water Pollutants Chemical}
}

@inproceedings{kalinin3DCellNuclear2018,
  title = {{{3D Cell Nuclear Morphology}}: {{Microscopy Imaging Dataset}} and {{Voxel-Based Morphometry Classification Results}}},
  shorttitle = {{{3D Cell Nuclear Morphology}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Kalinin, Alexandr A. and {Allyn-Feuer}, Ari and Ade, Alex and Fon, Gordon-Victor and Meixner, Walter and Dilworth, David and {de Wet}, Jeffrey R. and Higgins, Gerald A. and Zheng, Gen and Creekmore, Amy and Wiley, John W. and Verdone, James E. and Veltri, Robert W. and Pienta, Kenneth J. and Coffey, Donald S. and Athey, Brian D. and Dinov, Ivo D.},
  year = {2018},
  pages = {2272--2280},
  urldate = {2025-05-13},
  file = {/home/amunoz/Zotero/storage/5K785794/Kalinin et al. - 2018 - 3D Cell Nuclear Morphology Microscopy Imaging Dataset and Voxel-Based Morphometry Classification Re.pdf}
}

@misc{kimSelfsupervisionAdvancesMorphological2023,
  title = {Self-Supervision Advances Morphological Profiling by Unlocking Powerful Image Representations},
  author = {Kim, Vladislav and Adaloglou, Nikolaos and Osterland, Marc and Morelli, Flavio M. and Halawa, Marah and K{\"o}nig, Tim and Gnutt, David and Zapata, Paula A. Marin},
  year = {2023},
  month = apr,
  publisher = {Bioinformatics},
  doi = {10.1101/2023.04.28.538691},
  urldate = {2025-05-21},
  abstract = {Abstract           Cell Painting is an image-based assay that offers valuable insights into drug mechanisms of action and off-target effects. However, traditional feature extraction tools such as CellProfiler are computationally intensive and require frequent parameter adjustments. Inspired by recent advances in AI, we trained self-supervised learning (SSL) models DINO, MAE, and SimCLR on subsets of the JUMP-CP dataset to obtain powerful image representations for Cell Painting. We assessed the reproducibility and biological relevance of SSL features and uncovered the critical factors influencing model performance, such as training set composition and domain-specific normalization techniques. Our best model (DINO) surpassed CellProfiler in drug target and gene family classification, significantly reducing computational time and costs. All SSL models showed remarkable generalizability without fine-tuning, outperforming CellProfiler on an unseen dataset of genetic perturbations. Our study demonstrates the effectiveness of SSL methods for morphological profiling, suggesting promising research directions for improving the analysis of related image modalities.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/9CHSCM6R/Kim et al. - 2023 - Self-supervision advances morphological profiling by unlocking powerful image representations.pdf}
}

@ARTICLE{Xu2024-mn,
  title     = "A whole-slide foundation model for digital pathology from
               real-world data",
  author    = "Xu, Hanwen and Usuyama, Naoto and Bagga, Jaspreet and Zhang,
               Sheng and Rao, Rajesh and Naumann, Tristan and Wong, Cliff and
               Gero, Zelalem and González, Javier and Gu, Yu and Xu, Yanbo and
               Wei, Mu and Wang, Wenhui and Ma, Shuming and Wei, Furu and Yang,
               Jianwei and Li, Chunyuan and Gao, Jianfeng and Rosemon, Jaylen
               and Bower, Tucker and Lee, Soohee and Weerasinghe, Roshanthi and
               Wright, Bill J and Robicsek, Ari and Piening, Brian and Bifulco,
               Carlo and Wang, Sheng and Poon, Hoifung",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  630,
  number    =  8015,
  pages     = "181--188",
  abstract  = "Digital pathology poses unique computational challenges, as a
               standard gigapixel slide may comprise tens of thousands of image
               tiles1-3. Prior models have often resorted to subsampling a small
               portion of tiles for each slide, thus missing the important
               slide-level context4. Here we present Prov-GigaPath, a
               whole-slide pathology foundation model pretrained on 1.3 billion
               256 × 256 pathology image tiles in 171,189 whole slides from
               Providence, a large US health network comprising 28 cancer
               centres. The slides originated from more than 30,000 patients
               covering 31 major tissue types. To pretrain Prov-GigaPath, we
               propose GigaPath, a novel vision transformer architecture for
               pretraining gigapixel pathology slides. To scale GigaPath for
               slide-level learning with tens of thousands of image tiles,
               GigaPath adapts the newly developed LongNet5 method to digital
               pathology. To evaluate Prov-GigaPath, we construct a digital
               pathology benchmark comprising 9 cancer subtyping tasks and 17
               pathomics tasks, using both Providence and TCGA data6. With
               large-scale pretraining and ultra-large-context modelling,
               Prov-GigaPath attains state-of-the-art performance on 25 out of
               26 tasks, with significant improvement over the second-best
               method on 18 tasks. We further demonstrate the potential of
               Prov-GigaPath on vision-language pretraining for pathology7,8 by
               incorporating the pathology reports. In sum, Prov-GigaPath is an
               open-weight foundation model that achieves state-of-the-art
               performance on various digital pathology tasks, demonstrating the
               importance of real-world data and whole-slide modelling.",
  month     =  jun,
  year      =  2024,
  language  = "en"
}


@MISC{10x-Genomics2023-el,
  title        = "{FFPE} Human Breast with Custom Add-on Panel, In Situ Gene
                  Expression dataset",
  author       = "{10x Genomics}",
  publisher    = "10x Genomics",
  month        =  jan,
  year         =  2023,
  howpublished = "Xenium Onboard Analysis v1.0.2"
}


@ARTICLE{Virshup2023-scverse,
  title     = "The scverse project provides a computational ecosystem for
               single-cell omics data analysis",
  author    = "Virshup, Isaac and Bredikhin, Danila and Heumos, Lukas and Palla,
               Giovanni and Sturm, Gregor and Gayoso, Adam and Kats, Ilia and
               Koutrouli, Mikaela and {Scverse Community} and Berger, Bonnie and
               Pe'er, Dana and Regev, Aviv and Teichmann, Sarah A and Finotello,
               Francesca and Wolf, F Alexander and Yosef, Nir and Stegle, Oliver
               and Theis, Fabian J",
  journal   = "Nat. Biotechnol.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  41,
  number    =  5,
  pages     = "604--606",
  month     =  may,
  year      =  2023,
  language  = "en"
}


@ARTICLE{Marconato2024-SpatialData,
  title     = "{SpatialData}: an open and universal data framework for spatial
               omics",
  author    = "Marconato, Luca and Palla, Giovanni and Yamauchi, Kevin A and
               Virshup, Isaac and Heidari, Elyas and Treis, Tim and Vierdag,
               Wouter-Michiel and Toth, Marcella and Stockhaus, Sonja and
               Shrestha, Rahul B and Rombaut, Benjamin and Pollaris, Lotte and
               Lehner, Laurens and Vöhringer, Harald and Kats, Ilia and Saeys,
               Yvan and Saka, Sinem K and Huber, Wolfgang and Gerstung, Moritz
               and Moore, Josh and Theis, Fabian J and Stegle, Oliver",
  journal   = "Nat. Methods",
  publisher = "Nature Publishing Group",
  pages     = "1--5",
  abstract  = "Spatially resolved omics technologies are transforming our
               understanding of biological tissues. However, the handling of
               uni- and multimodal spatial omics datasets remains a challenge
               owing to large data volumes, heterogeneity of data types and the
               lack of flexible, spatially aware data structures. Here we
               introduce SpatialData, a framework that establishes a unified and
               extensible multiplatform file-format, lazy representation of
               larger-than-memory data, transformations and alignment to common
               coordinate systems. SpatialData facilitates spatial annotations
               and cross-modal aggregation and analysis, the utility of which is
               illustrated in the context of multiple vignettes, including
               integrative analysis on a multimodal Xenium and Visium breast
               cancer study.",
  month     =  mar,
  year      =  2024,
  language  = "en"
}


@ARTICLE{Chen2024-ix,
  title     = "Towards a general-purpose foundation model for computational
               pathology",
  author    = "Chen, Richard J and Ding, Tong and Lu, Ming Y and Williamson,
               Drew F K and Jaume, Guillaume and Song, Andrew H and Chen, Bowen
               and Zhang, Andrew and Shao, Daniel and Shaban, Muhammad and
               Williams, Mane and Oldenburg, Lukas and Weishaupt, Luca L and
               Wang, Judy J and Vaidya, Anurag and Le, Long Phi and Gerber,
               Georg and Sahai, Sharifa and Williams, Walt and Mahmood, Faisal",
  journal   = "Nat. Med.",
  publisher = "Nature Publishing Group",
  volume    =  30,
  number    =  3,
  pages     = "850--862",
  abstract  = "Quantitative evaluation of tissue images is crucial for
               computational pathology (CPath) tasks, requiring the objective
               characterization of histopathological entities from whole-slide
               images (WSIs). The high resolution of WSIs and the variability of
               morphological features present significant challenges,
               complicating the large-scale annotation of data for
               high-performance applications. To address this challenge, current
               efforts have proposed the use of pretrained image encoders
               through transfer learning from natural image datasets or
               self-supervised learning on publicly available histopathology
               datasets, but have not been extensively developed and evaluated
               across diverse tissue types at scale. We introduce UNI, a
               general-purpose self-supervised model for pathology, pretrained
               using more than 100 million images from over 100,000 diagnostic
               H\&E-stained WSIs (>77 TB of data) across 20 major tissue types.
               The model was evaluated on 34 representative CPath tasks of
               varying diagnostic difficulty. In addition to outperforming
               previous state-of-the-art models, we demonstrate new modeling
               capabilities in CPath such as resolution-agnostic tissue
               classification, slide classification using few-shot class
               prototypes, and disease subtyping generalization in classifying
               up to 108 cancer types in the OncoTree classification system. UNI
               advances unsupervised representation learning at scale in CPath
               in terms of both pretraining data and downstream evaluation,
               enabling data-efficient artificial intelligence models that can
               generalize and transfer to a wide range of diagnostically
               challenging tasks and clinical workflows in anatomic pathology.",
  month     =  mar,
  year      =  2024,
  language  = "en"
}


@inproceedings{lafargeCapturingSingleCellPhenotypic2019,
  title = {Capturing {{Single-Cell Phenotypic Variation}} via {{Unsupervised Representation Learning}}},
  booktitle = {Proceedings of {{The}} 2nd {{International Conference}} on {{Medical Imaging}} with {{Deep Learning}}},
  author = {Lafarge, Maxime W. and Caicedo, Juan C. and Carpenter, Anne E. and Pluim, Josien P. W. and Singh, Shantanu and Veta, Mitko},
  year = {2019},
  month = may,
  pages = {315--325},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-21},
  abstract = {We propose a novel variational autoencoder (VAE) framework for learning representations of cell images for the domain of image-based profiling, important for new therapeutic discovery. Previously, generative adversarial network-based (GAN) approaches were proposed to enable biologists to visualize structural variations in cells that drive differences in populations. However, while the images were realistic, they did not provide direct reconstructions from representations, and their performance in downstream analysis was poor.  We address these limitations in our approach by adding an adversarial-driven similarity constraint applied to the standard VAE framework, and a progressive training procedure that allows higher quality reconstructions than standard VAE's. The proposed models improve classification accuracy by 22\% (to 90\%) compared to the best reported GAN model, making it competitive with other models that have higher quality representations, but lack the ability to synthesize images. This provides researchers a new tool to match cellular phenotypes effectively, and also to gain better insight into cellular structure variations that are driving differences between populations of cells.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/2JJYYIST/Lafarge et al. - 2019 - Capturing Single-Cell Phenotypic Variation via Unsupervised Representation Learning.pdf}
}

@inproceedings{lamNumbaLLVMbasedPython2015,
  title = {Numba: A {{LLVM-based Python JIT}} Compiler},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  year = {2015},
  month = nov,
  series = {{{LLVM}} '15},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2833157.2833162},
  urldate = {2025-05-13},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2},
  file = {/home/amunoz/Zotero/storage/YD35Y5F6/Lam et al. - 2015 - Numba a LLVM-based Python JIT compiler.pdf}
}

@article{liChallengesOpportunitiesBioimage2023,
  title = {Challenges and Opportunities in Bioimage Analysis},
  author = {Li, Xinyang and Zhang, Yuanlong and Wu, Jiamin and Dai, Qionghai},
  year = {2023},
  month = jul,
  journal = {Nature Methods},
  volume = {20},
  number = {7},
  pages = {958--961},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-023-01900-4},
  urldate = {2025-05-21},
  abstract = {Advanced imaging techniques provide holistic observations of complicated biological phenomena across multiple scales while posing great challenges to data analysis. We summarize recent advances and trends in bioimage analysis, discuss current challenges toward better applicability, and envisage new possibilities.},
  copyright = {2023 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Biotechnology,Software},
  file = {/home/amunoz/Zotero/storage/52C6WA9M/Li et al. - 2023 - Challenges and opportunities in bioimage analysis.pdf}
}

@article{marconatoSpatialDataOpenUniversal2025,
  title = {{{SpatialData}}: An Open and Universal Data Framework for Spatial Omics},
  shorttitle = {{{SpatialData}}},
  author = {Marconato, Luca and Palla, Giovanni and Yamauchi, Kevin A. and Virshup, Isaac and Heidari, Elyas and Treis, Tim and Vierdag, Wouter-Michiel and Toth, Marcella and Stockhaus, Sonja and Shrestha, Rahul B. and Rombaut, Benjamin and Pollaris, Lotte and Lehner, Laurens and V{\"o}hringer, Harald and Kats, Ilia and Saeys, Yvan and Saka, Sinem K. and Huber, Wolfgang and Gerstung, Moritz and Moore, Josh and Theis, Fabian J. and Stegle, Oliver},
  year = {2025},
  month = jan,
  journal = {Nature Methods},
  volume = {22},
  number = {1},
  pages = {58--62},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02212-x},
  urldate = {2025-05-20},
  abstract = {Spatially resolved omics technologies are transforming our understanding of biological tissues. However, the handling of uni- and multimodal spatial omics datasets remains a challenge owing to large data volumes, heterogeneity of data types and the lack of flexible, spatially aware data structures. Here we introduce SpatialData, a framework that establishes a unified and extensible multiplatform file-format, lazy representation of larger-than-memory data, transformations and alignment to common coordinate systems. SpatialData facilitates spatial annotations and cross-modal aggregation and analysis, the utility of which is illustrated in the context of multiple vignettes, including integrative analysis on a multimodal Xenium and Visium breast cancer study.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational platforms and environments,Data integration,Molecular imaging,Software},
  file = {/home/amunoz/Zotero/storage/KU5VUM2H/Marconato et al. - 2025 - SpatialData an open and universal data framework for spatial omics.pdf}
}

@article{mcquinCellProfiler30Nextgeneration2018,
  title = {{{CellProfiler}} 3.0: {{Next-generation}} Image Processing for Biology},
  shorttitle = {{{CellProfiler}} 3.0},
  author = {McQuin, Claire and Goodman, Allen and Chernyshev, Vasiliy and Kamentsky, Lee and Cimini, Beth A. and Karhohs, Kyle W. and Doan, Minh and Ding, Liya and Rafelski, Susanne M. and Thirstrup, Derek and Wiegraebe, Winfried and Singh, Shantanu and Becker, Tim and Caicedo, Juan C. and Carpenter, Anne E.},
  year = {2018},
  month = jul,
  journal = {PLOS Biology},
  volume = {16},
  number = {7},
  pages = {e2005970},
  publisher = {Public Library of Science},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2005970},
  urldate = {2025-05-08},
  abstract = {CellProfiler has enabled the scientific research community to create flexible, modular image analysis pipelines since its release in 2005. Here, we describe CellProfiler 3.0, a new version of the software supporting both whole-volume and plane-wise analysis of three-dimensional (3D) image stacks, increasingly common in biomedical research. CellProfiler's infrastructure is greatly improved, and we provide a protocol for cloud-based, large-scale image processing. New plugins enable running pretrained deep learning models on images. Designed by and for biologists, CellProfiler equips researchers with powerful computational tools via a well-documented user interface, empowering biologists in all fields to create quantitative, reproducible image analysis workflows.},
  langid = {english},
  keywords = {Biologists,Blastocysts,Cell staining,Computer software,Deep learning,Image analysis,Image processing,Open source software},
  file = {/home/amunoz/Zotero/storage/ZHFPXNFQ/McQuin et al. - 2018 - CellProfiler 3.0 Next-generation image processing for biology.pdf}
}

@article{moenDeepLearningCellular2019,
  title = {Deep Learning for Cellular Image Analysis},
  author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
  year = {2019},
  month = dec,
  journal = {Nature Methods},
  volume = {16},
  number = {12},
  pages = {1233--1246},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0403-1},
  urldate = {2025-05-08},
  abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
  copyright = {2019 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Image processing,Software},
  file = {/home/amunoz/Zotero/storage/ZEI8L6EP/Moen et al. - 2019 - Deep learning for cellular image analysis.pdf}
}

@misc{moshkovLearningRepresentationsImagebased2022,
  title = {Learning Representations for Image-Based Profiling of Perturbations},
  author = {Moshkov, Nikita and Bornholdt, Michael and Benoit, Santiago and Smith, Matthew and McQuin, Claire and Goodman, Allen and Senft, Rebecca A. and Han, Yu and Babadi, Mehrtash and Horvath, Peter and Cimini, Beth A. and Carpenter, Anne E. and Singh, Shantanu and Caicedo, Juan C.},
  year = {2022},
  month = aug,
  publisher = {Bioinformatics},
  doi = {10.1101/2022.08.12.503783},
  urldate = {2025-05-21},
  abstract = {Abstract                        Measuring the phenotypic effect of treatments on cells through imaging assays is an efficient and powerful way of studying cell biology, and requires computational methods for transforming images into quantitative data that highlight phenotypic outcomes. Here, we present an optimized strategy for learning representations of treatment effects from high-throughput imaging data, which follows a causal framework for interpreting results and guiding performance improvements. We use weakly supervised learning (WSL) for modeling associations between images and treatments, and show that it encodes both confounding factors and phenotypic features in the learned representation. To facilitate their separation, we constructed a large training dataset with Cell Painting images from five different studies to maximize experimental diversity, following insights from our causal analysis. Training a WSL model with this dataset successfully improves downstream performance, and produces a reusable convolutional network for image-based profiling, which we call             Cell Painting CNN-1             . We conducted a comprehensive evaluation of our strategy on three publicly available Cell Painting datasets, discovering that representations obtained by the Cell Painting CNN-1 can improve performance in downstream analysis for biological matching up to 30\% with respect to classical features, while also being more computationally efficient.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/3A66BEYA/Moshkov et al. - 2022 - Learning representations for image-based profiling of perturbations.pdf}
}

@article{pachitariuCellpose20How2022,
  title = {Cellpose 2.0: How to Train Your Own Model},
  shorttitle = {Cellpose 2.0},
  author = {Pachitariu, Marius and Stringer, Carsen},
  year = {2022},
  month = dec,
  journal = {Nature Methods},
  volume = {19},
  number = {12},
  pages = {1634--1641},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-022-01663-4},
  urldate = {2025-05-06},
  abstract = {Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500--1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100--200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational platforms and environments,Image processing},
  file = {/home/amunoz/Zotero/storage/B6TZE2BA/Pachitariu and Stringer - 2022 - Cellpose 2.0 how to train your own model.pdf}
}

@article{pallaSquidpyScalableFramework2022,
  title = {Squidpy: A Scalable Framework for Spatial Omics Analysis},
  shorttitle = {Squidpy},
  author = {Palla, Giovanni and Spitzer, Hannah and Klein, Michal and Fischer, David and Schaar, Anna Christina and Kuemmerle, Louis Benedikt and Rybakov, Sergei and Ibarra, Ignacio L. and Holmberg, Olle and Virshup, Isaac and Lotfollahi, Mohammad and Richter, Sabrina and Theis, Fabian J.},
  year = {2022},
  month = feb,
  journal = {Nature Methods},
  volume = {19},
  number = {2},
  pages = {171--178},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01358-2},
  urldate = {2025-05-20},
  abstract = {Spatial omics data are advancing the study of tissue organization and cellular communication at an unprecedented scale. Flexible tools are required to store, integrate and visualize the large diversity of spatial omics data. Here, we present Squidpy, a Python framework that brings together tools from omics and image analysis to enable scalable description of spatial molecular data, such as transcriptome or multivariate proteins. Squidpy provides efficient infrastructure and numerous analysis methods that allow to efficiently store, manipulate and interactively visualize spatial omics data. Squidpy is extensible and can be interfaced with a variety of already existing libraries for the scalable analysis of spatial omics data.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Data integration,Imaging,Software,Transcriptomics},
  file = {/home/amunoz/Zotero/storage/WEFJJYTG/Palla et al. - 2022 - Squidpy a scalable framework for spatial omics analysis.pdf}
}

@misc{pawlowskiAutomatingMorphologicalProfiling2016,
  title = {Automating {{Morphological Profiling}} with {{Generic Deep Convolutional Networks}}},
  author = {Pawlowski, Nick and Caicedo, Juan C and Singh, Shantanu and Carpenter, Anne E and Storkey, Amos},
  year = {2016},
  month = nov,
  publisher = {Bioinformatics},
  doi = {10.1101/085118},
  urldate = {2025-05-21},
  abstract = {Abstract           Morphological profiling aims to create signatures of genes, chemicals and diseases from microscopy images. Current approaches use classical computer vision-based segmentation and feature extraction. Deep learning models achieve state-of-the-art performance in many computer vision tasks such as classification and segmentation. We propose to transfer activation features of generic deep convolutional networks to extract features for morphological profiling. Our approach surpasses currently used methods in terms of accuracy and processing speed. Furthermore, it enables fully automated processing of microscopy images without need for single cell identification.},
  archiveprefix = {Bioinformatics},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/3I2YDUNC/Pawlowski et al. - 2016 - Automating Morphological Profiling with Generic Deep Convolutional Networks.pdf}
}

@article{sealDecadeSystematicReview2024,
  title = {A {{Decade}} in a {{Systematic Review}}: {{The Evolution}} and {{Impact}} of {{Cell Painting}}},
  shorttitle = {A {{Decade}} in a {{Systematic Review}}},
  author = {Seal, Srijit and Trapotsi, Maria-Anna and Spjuth, Ola and Singh, Shantanu and {Carreras-Puigvert}, Jordi and Greene, Nigel and Bender, Andreas and Carpenter, Anne E.},
  year = {2024},
  month = may,
  journal = {bioRxiv},
  pages = {2024.05.04.592531},
  issn = {2692-8205},
  doi = {10.1101/2024.05.04.592531},
  urldate = {2025-05-13},
  abstract = {High-content image-based assays have fueled significant discoveries in the life sciences in the past decade (2013--2023), including novel insights into disease etiology, mechanism of action, new therapeutics, and toxicology predictions. Here, we systematically review the substantial methodological advancements and applications of Cell Painting. Advancements include improvements in the Cell Painting protocol, assay adaptations for different types of perturbations and applications, and improved methodologies for feature extraction, quality control, and batch effect correction. Moreover, machine learning methods recently surpassed classical approaches in their ability to extract biologically useful information from Cell Painting images. Cell Painting data have been used alone or in combination with other - omics data to decipher the mechanism of action of a compound, its toxicity profile, and many other biological effects. Overall, key methodological advances have expanded Cell Painting's ability to capture cellular responses to various perturbations. Future advances will likely lie in advancing computational and experimental techniques, developing new publicly available datasets, and integrating them with other high-content data types.},
  pmcid = {PMC11100607},
  pmid = {38766203},
  file = {/home/amunoz/Zotero/storage/4IJP67CI/Seal et al. - 2024 - A Decade in a Systematic Review The Evolution and Impact of Cell Painting.pdf}
}

@article{serranoReproducibleImagebasedProfiling2025,
  title = {Reproducible Image-Based Profiling with {{Pycytominer}}},
  author = {Serrano, Erik and Chandrasekaran, Srinivas Niranj and Bunten, Dave and Brewer, Kenneth I. and Tomkinson, Jenna and Kern, Roshan and Bornholdt, Michael and Fleming, Stephen J. and Pei, Ruifan and Arevalo, John and Tsang, Hillary and Rubinetti, Vincent and {Tromans-Coia}, Callum and Becker, Tim and Weisbart, Erin and Bunne, Charlotte and Kalinin, Alexandr A. and Senft, Rebecca and Taylor, Stephen J. and Jamali, Nasim and Adeboye, Adeniyi and Abbasi, Hamdah Shafqat and Goodman, Allen and Caicedo, Juan C. and Carpenter, Anne E. and Cimini, Beth A. and Singh, Shantanu and Way, Gregory P.},
  year = {2025},
  month = apr,
  journal = {Nature Methods},
  volume = {22},
  number = {4},
  pages = {677--680},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-025-02611-8},
  urldate = {2025-05-13},
  abstract = {Advances in high-throughput microscopy have enabled the rapid acquisition of large numbers of high-content microscopy images. Next, whether by deep learning or classical algorithms, image analysis pipelines commonly produce single-cell features. To process these single cells for downstream applications, we present Pycytominer, a user-friendly, open-source Python package that implements the bioinformatics steps key to image-based profiling. We demonstrate Pycytominer's usefulness in a machine-learning project to predict nuisance compounds that cause undesirable cell injuries.},
  copyright = {2025 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {High-throughput screening,Machine learning,Software},
  file = {/home/amunoz/Zotero/storage/59E67ZGT/Serrano et al. - 2025 - Reproducible image-based profiling with Pycytominer.pdf}
}

@article{stirlingCellProfiler4Improvements2021,
  title = {{{CellProfiler}} 4: Improvements in Speed, Utility and Usability},
  shorttitle = {{{CellProfiler}} 4},
  author = {Stirling, David R. and {Swain-Bowden}, Madison J. and Lucas, Alice M. and Carpenter, Anne E. and Cimini, Beth A. and Goodman, Allen},
  year = {2021},
  month = sep,
  journal = {BMC Bioinformatics},
  volume = {22},
  number = {1},
  pages = {433},
  issn = {1471-2105},
  doi = {10.1186/s12859-021-04344-9},
  urldate = {2025-05-05},
  abstract = {Imaging data contains a substantial amount of information which can be difficult to evaluate by eye. With the expansion of high throughput microscopy methodologies producing increasingly large datasets, automated and objective analysis of the resulting images is essential to effectively extract biological information from this data. CellProfiler is a free, open source image analysis program which enables researchers to generate modular pipelines with which to process microscopy images into interpretable measurements.},
  langid = {english},
  keywords = {Bioimaging,Image analysis,Image quantitation,Image segmentation,Microscopy},
  file = {/home/amunoz/Zotero/storage/8D496EM6/Stirling et al. - 2021 - CellProfiler 4 improvements in speed, utility and usability.pdf}
}

@article{stringerCellposeGeneralistAlgorithm2021,
  title = {Cellpose: A Generalist Algorithm for Cellular Segmentation},
  shorttitle = {Cellpose},
  author = {Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
  year = {2021},
  month = jan,
  journal = {Nature Methods},
  volume = {18},
  number = {1},
  pages = {100--106},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-020-01018-x},
  urldate = {2025-05-21},
  abstract = {Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cell biology,Computational biology and bioinformatics},
  file = {/home/amunoz/Zotero/storage/RPUU3WXI/Stringer et al. - 2021 - Cellpose a generalist algorithm for cellular segmentation.pdf}
}

@inproceedings{sundararajanManyShapleyValues2020,
  title = {The {{Many Shapley Values}} for {{Model Explanation}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Sundararajan, Mukund and Najmi, Amir},
  year = {2020},
  month = nov,
  pages = {9269--9278},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-20},
  abstract = {The Shapley value has become the basis for several methods that attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing the uniqueness result from~{\textbackslash}cite\{Shapley53\}, which shows that it is the only method that satisfies certain good properties ({\textbackslash}emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized for model explanation. These differ in how they reference the model, the training data, and the explanation context. Hence they differ in output, rendering the uniqueness result inapplicable. Furthermore, the techniques that rely on they training data produce non-intuitive attributions, for instance unused features can still receive attribution. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution. We discuss a technique called Baseline Shapley (BShap), provide a proper uniqueness result for it, and contrast it with two other techniques from prior literature, Integrated Gradients~{\textbackslash}cite\{STY17\} and Conditional Expectation Shapley~{\textbackslash}cite\{Lundberg2017AUA\}.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/XY2MJGJD/Sundararajan and Najmi - 2020 - The Many Shapley Values for Model Explanation.pdf}
}

@article{tangMorphologicalProfilingDrug2024,
  title = {Morphological Profiling for Drug Discovery in the Era of Deep Learning},
  author = {Tang, Qiaosi and Ratnayake, Ranjala and Seabra, Gustavo and Jiang, Zhe and Fang, Ruogu and Cui, Lina and Ding, Yousong and Kahveci, Tamer and Bian, Jiang and Li, Chenglong and Luesch, Hendrik and Li, Yanjun},
  year = {2024},
  month = jul,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {4},
  pages = {bbae284},
  issn = {1477-4054},
  doi = {10.1093/bib/bbae284},
  urldate = {2025-05-08},
  abstract = {Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high throughput. These efforts have facilitated understanding of compound mechanism of action, drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering-- and deep learning--based approaches, and introduce publicly available benchmark datasets. We place a particular emphasis on the application of deep learning in this pipeline, covering cell segmentation, image representation learning, and multimodal learning. Additionally, we illuminate the application of morphological profiling in phenotypic drug discovery and highlight potential challenges and opportunities in this field.},
  file = {/home/amunoz/Zotero/storage/YBYS8LNL/Tang et al. - 2024 - Morphological profiling for drug discovery in the era of deep learning.pdf;/home/amunoz/Zotero/storage/W5LLGRM8/7693952.html}
}

@article{waltScikitimageImageProcessing2014,
  title = {Scikit-Image: Image Processing in {{Python}}},
  shorttitle = {Scikit-Image},
  author = {van der Walt, St{\'e}fan and Sch{\"o}nberger, Johannes L. and {Nunez-Iglesias}, Juan and Boulogne, Fran{\c c}ois and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  year = {2014},
  month = jun,
  journal = {PeerJ},
  volume = {2},
  pages = {e453},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.453},
  urldate = {2025-05-08},
  abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/FT4SJ6TM/Walt et al. - 2014 - scikit-image image processing in Python.pdf}
}

@article{weisbartCellPaintingGallery2024,
  title = {Cell {{Painting Gallery}}: An Open Resource for Image-Based Profiling},
  shorttitle = {Cell {{Painting Gallery}}},
  author = {Weisbart, Erin and Kumar, Ankur and Arevalo, John and Carpenter, Anne E. and Cimini, Beth A. and Singh, Shantanu},
  year = {2024},
  month = oct,
  journal = {Nature Methods},
  volume = {21},
  number = {10},
  pages = {1775--1777},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02399-z},
  urldate = {2025-05-21},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cellular imaging,Data publication and archiving,Databases,High-throughput screening,Image processing},
  file = {/home/amunoz/Zotero/storage/PMEXZTMF/Weisbart et al. - 2024 - Cell Painting Gallery an open resource for image-based profiling.pdf}
}

@ARTICLE{Traag2019-fr,
  title     = "From Louvain to Leiden: guaranteeing well-connected communities",
  author    = "Traag, V A and Waltman, L and van Eck, N J",
  journal   = "Sci. Rep.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  9,
  number    =  1,
  pages     =  5233,
  abstract  = "Community detection is often used to understand the structure of
               large and complex networks. One of the most popular algorithms
               for uncovering community structure is the so-called Louvain
               algorithm. We show that this algorithm has a major defect that
               largely went unnoticed until now: the Louvain algorithm may yield
               arbitrarily badly connected communities. In the worst case,
               communities may even be disconnected, especially when running the
               algorithm iteratively. In our experimental analysis, we observe
               that up to 25\% of the communities are badly connected and up to
               16\% are disconnected. To address this problem, we introduce the
               Leiden algorithm. We prove that the Leiden algorithm yields
               communities that are guaranteed to be connected. In addition, we
               prove that, when the Leiden algorithm is applied iteratively, it
               converges to a partition in which all subsets of all communities
               are locally optimally assigned. Furthermore, by relying on a fast
               local move approach, the Leiden algorithm runs faster than the
               Louvain algorithm. We demonstrate the performance of the Leiden
               algorithm for several benchmark and real-world networks. We find
               that the Leiden algorithm is faster than the Louvain algorithm
               and uncovers better partitions, in addition to providing explicit
               guarantees.",
  month     =  mar,
  year      =  2019,
  language  = "en"
}


@ARTICLE{Wu2024-zp,
  title     = "Cold and hot tumors: from molecular mechanisms to targeted
               therapy",
  author    = "Wu, Bo and Zhang, Bo and Li, Bowen and Wu, Haoqi and Jiang, Meixi",
  journal   = "Signal Transduct. Target. Ther.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  9,
  number    =  1,
  pages     =  274,
  abstract  = "Immunotherapy has made significant strides in cancer treatment,
               particularly through immune checkpoint blockade (ICB), which has
               shown notable clinical benefits across various tumor types.
               Despite the transformative impact of ICB treatment in cancer
               therapy, only a minority of patients exhibit a positive response
               to it. In patients with solid tumors, those who respond well to
               ICB treatment typically demonstrate an active immune profile
               referred to as the ``hot'' (immune-inflamed) phenotype. On the
               other hand, non-responsive patients may exhibit a distinct
               ``cold'' (immune-desert) phenotype, differing from the features
               of ``hot'' tumors. Additionally, there is a more nuanced
               ``excluded'' immune phenotype, positioned between the ``cold''
               and ``hot'' categories, known as the immune ``excluded'' type.
               Effective differentiation between ``cold'' and ``hot'' tumors,
               and understanding tumor intrinsic factors, immune
               characteristics, TME, and external factors are critical for
               predicting tumor response and treatment results. It is widely
               accepted that ICB therapy exerts a more profound effect on
               ``hot'' tumors, with limited efficacy against ``cold'' or
               ``altered'' tumors, necessitating combinations with other
               therapeutic modalities to enhance immune cell infiltration into
               tumor tissue and convert ``cold'' or ``altered'' tumors into
               ``hot'' ones. Therefore, aligning with the traits of ``cold'' and
               ``hot'' tumors, this review systematically delineates the
               respective immune characteristics, influencing factors, and
               extensively discusses varied treatment approaches and drug
               targets based on ``cold'' and ``hot'' tumors to assess clinical
               efficacy.",
  month     =  oct,
  year      =  2024,
  language  = "en"
}


@article{wolfSCANPYLargescaleSinglecell2018,
  title = {{{SCANPY}}: Large-Scale Single-Cell Gene Expression Data Analysis},
  shorttitle = {{{SCANPY}}},
  author = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
  year = {2018},
  month = feb,
  journal = {Genome Biology},
  volume = {19},
  number = {1},
  pages = {15},
  issn = {1474-760X},
  doi = {10.1186/s13059-017-1382-0},
  urldate = {2025-05-20},
  abstract = {Scanpy is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with Scanpy, we present AnnData, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).},
  langid = {english},
  keywords = {Bioinformatics,Clustering,Differential expression testing,Gene Expression Analysis,Gene expression profiling,Genome-wide analysis of gene expression,Graph analysis,High-throughput Cell Screening,Machine learning,Microarray analysis,Pseudotemporal ordering,Scalability,Single-cell transcriptomics,Trajectory inference,Transcriptomics,Visualization},
  file = {/home/amunoz/Zotero/storage/9Z7V9KWX/Wolf et al. - 2018 - SCANPY large-scale single-cell gene expression data analysis.pdf}
}

@article{wongDeepRepresentationLearning2023,
  title = {Deep Representation Learning Determines Drug Mechanism of Action from Cell Painting Images},
  author = {Wong, Daniel R. and Logan, David J. and Hariharan, Santosh and Stanton, Robert and Clevert, Djork-Arn{\'e} and Kiruluta, Andrew},
  year = {2023},
  journal = {Digital Discovery},
  volume = {2},
  number = {5},
  pages = {1354--1367},
  issn = {2635-098X},
  doi = {10.1039/D3DD00060E},
  urldate = {2025-05-21},
  abstract = {Fluorescent-based microscopy screens carry a broad range of phenotypic information about how compounds affect cellular biology.           ,                             Fluorescent-based microscopy screens carry a broad range of phenotypic information about how compounds affect cellular biology. From changes in cellular morphology observed in these screens, one key area of medicinal interest is determining a compound's mechanism of action. However, much of this phenotypic information is subtle and difficult to quantify. Hence, creating quantitative embeddings that can measure cellular response to compound perturbation has been a key area of research. Here we present a deep learning enabled encoder called MOAProfiler that captures phenotypic features for determining mechanism of action from Cell Painting images. We compared our method with both the traditional gold-standard means of feature encoding               via               CellProfiler and a deep learning encoder called DeepProfiler. The results, on two independent and biologically different datasets, indicated that MOAProfiler encoded MOA-specific features that allowed for more accurate clustering and classification of compounds over hundreds of different MOAs.},
  langid = {english},
  file = {/home/amunoz/Zotero/storage/W7A7MTX6/Wong et al. - 2023 - Deep representation learning determines drug mechanism of action from cell painting images.pdf}
}

@article{kalininValproicAcidinducedChanges2021,
  title = {Valproic Acid-Induced Changes of {{4D}} Nuclear Morphology in Astrocyte Cells},
  author = {Kalinin, Alexandr A. and Hou, Xinhai and Ade, Alex S. and Fon, Gordon-Victor and Meixner, Walter and Higgins, Gerald A. and Sexton, Jonathan Z. and Wan, Xiang and Dinov, Ivo D. and O'Meara, Matthew J. and Athey, Brian D.},
  year = {2021},
  month = aug,
  journal = {Molecular Biology of the Cell},
  volume = {32},
  number = {18},
  pages = {1624--1633},
  publisher = {American Society for Cell Biology (mboc)},
  issn = {1059-1524},
  doi = {10.1091/mbc.E20-08-0502},
  urldate = {2025-05-26},
  abstract = {Histone deacetylase inhibitors, such as valproic acid (VPA), have important clinical therapeutic and cellular reprogramming applications. They induce chromatin reorganization that is associated with altered cellular morphology. However, there is a lack of comprehensive characterization of VPA-induced changes of nuclear size and shape. Here, we quantify 3D nuclear morphology of primary human astrocyte cells treated with VPA over time (hence, 4D). We compared volumetric and surface-based representations and identified seven features that jointly discriminate between normal and treated cells with 85\% accuracy on day 7. From day 3, treated nuclei were more elongated and flattened and then continued to morphologically diverge from controls over time, becoming larger and more irregular. On day 7, most of the size and shape descriptors demonstrated significant differences between treated and untreated cells, including a 24\% increase in volume and 6\% reduction in extent (shape regularity) for treated nuclei. Overall, we show that 4D morphometry can capture how chromatin reorganization modulates the size and shape of the nucleus over time. These nuclear structural alterations may serve as a biomarker for histone (de-)acetylation events and provide insights into mechanisms of astrocytes-to-neurons reprogramming.},
  file = {/Users/amunozgo/Zotero/storage/GD3GYE3N/Kalinin et al. - 2021 - Valproic acid-induced changes of 4D nuclear morpho.pdf}
}

@article{chandrasekaranImagebasedProfilingDrug2021,
  title = {Image-Based Profiling for Drug Discovery: Due for a Machine-Learning Upgrade?},
  shorttitle = {Image-Based Profiling for Drug Discovery},
  author = {Chandrasekaran, Srinivas Niranj and Ceulemans, Hugo and Boyd, Justin D. and Carpenter, Anne E.},
  year = {2021},
  month = feb,
  journal = {Nature Reviews Drug Discovery},
  volume = {20},
  number = {2},
  pages = {145--159},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/s41573-020-00117-w},
  urldate = {2023-08-17},
  abstract = {Image-based profiling is a maturing strategy by which the rich information present in biological images is reduced to a multidimensional profile, a collection of extracted image-based features. These profiles can be mined for relevant patterns, revealing unexpected biological activity that is useful for many steps in the drug discovery process. Such applications include identifying disease-associated screenable phenotypes, understanding disease mechanisms and predicting a drug's activity, toxicity or mechanism of action. Several of these applications have been recently validated and have moved into production mode within academia and the pharmaceutical industry. Some of these have yielded disappointing results in practice but are now of renewed interest due to improved machine-learning strategies that better leverage image-based information. Although challenges remain, novel computational technologies such as deep learning and single-cell methods that better capture the biological information in images hold promise for accelerating drug discovery.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Phenotypic screening},
  file = {/Users/amunozgo/Zotero/storage/X7FTDWFV/Chandrasekaran et al. - 2021 - Image-based profiling for drug discovery due for .pdf}
}


@article{xuDesignNonDestructiveSeed2024,
  title = {Design of a {{Non-Destructive Seed Counting Instrument}} for {{Rapeseed Pods Based}} on {{Transmission Imaging}}},
  author = {Xu, Shengyong and Xu, Rongsheng and Ma, Pan and Huang, Zhenhao and Wang, Shaodong and Yang, Zhe and Liao, Qingxi},
  year = {2024},
  month = dec,
  journal = {Agriculture},
  volume = {14},
  number = {12},
  pages = {2215},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2077-0472},
  doi = {10.3390/agriculture14122215},
  urldate = {2025-05-26},
  abstract = {Pod counting of rapeseed is a critical step in breeding, cultivation, and agricultural machinery research. Currently, this process relies entirely on manual labor, which is both labor-intensive and inefficient. This study aims to develop a semi-automatic counting instrument based on transmission image processing and proposes a new algorithm for processing transmission images of pods to achieve non-destructive, accurate, and rapid determination of the seed count per pod. Initially, the U-NET network was used to segment and remove the stem and beak from the pod image; subsequently, adaptive contrast enhancement was applied to adjust the contrast of the G-channel image of the pod to an appropriate range, effectively eliminating the influence of different varieties and maturity levels on the translucency of the pod skin. After enhancing the contrast, the Sauvola algorithm was employed for threshold segmentation to remove the pod skin, followed by thinning and dilation of the binary image to extract and remove the central ridge lines, detecting the number and area of connected domains. Finally, the seed count was determined based on the ratio of each connected domain's area to the mean area of all connected domains. A transmission imaging device that mimics the human eye's method of counting seeds was designed, incorporating an LED transmission light source, photoelectric switch-triggered imaging slot, an industrial camera, and an integrated packaging frame. Human--machine interaction software based on PyQt5 was developed, integrating functions such as communication between upper and lower machines, image acquisition, storage, and processing. Operators simply need to place the pod in an upright position into the imaging device, where its transmission image will be automatically captured and processed. The results are displayed on a touchscreen and stored in Excel spreadsheets. The experimental results show that the instrument is accurate, user-friendly, and significantly reduces labor intensity. For various varieties of rapeseed pods, the seed counting accuracy reached 97.2\% with a throughput of 372 pods/h, both of which are significantly better than manual counting and have considerable potential for practical applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,image processing,rape,rapeseed,seed counting,transmitted light source,yield measurement},
  file = {/Users/amunozgo/Zotero/storage/GCGQFFZD/Xu et al. - 2024 - Design of a Non-Destructive Seed Counting Instrument for Rapeseed Pods Based on Transmission Imaging.pdf}
}


@article{ilhanEffectsProcessParameters2021,
  title = {Effects of Process Parameters and Solid Particle Contaminants on the Seal Strength of Low-Density Polyethylene-Based Flexible Food Packaging Films},
  author = {Ilhan, Ilknur and {ten Klooster}, Roland and Gibson, Ian},
  year = {2021},
  journal = {Packaging Technology and Science},
  volume = {34},
  number = {7},
  pages = {413--421},
  issn = {1099-1522},
  doi = {10.1002/pts.2567},
  urldate = {2025-05-26},
  abstract = {Seal strength is a key indicator of heat seal quality in flexible packaging. In this study, the effect of seal bar geometry, material composition and food particle contamination on the seal strength of widely used low-density polyethylene (LDPE)-based compound films was examined. Additionally, the maximum level of allowable solid food particle contamination was determined for ground coffee particles and powdered sugar. The results showed that adding metallocene LLDPE compound decreases seal initiation temperature (SIT) and increases overall seal strength. Also, changing seal bar geometry from flat to grooved bar with 0.56-mm pitch height enhanced the seal strength significantly. Moreover, pressure mapping and T-peel tests at SIT pointed out that grooved bars alter the pressure distribution and first contact points through the seal surface. Contamination of ground coffee particles at the seal interface as occurs during the packaging process when a powdery product is dropped in a package did not affect the seal strength up to 10 g/m2 at 0.5-s dwell time. Above that amount, seal strength dropped dramatically. In the case of powdered sugar, threshold contaminant level was 2 g/m2 at 0.5-s dwell time. Consequently, it has been revealed that knowing the type and the amount of contaminant during the food packaging process is important to maintain the seal quality, to find optimum process values between dosing and filling and to choose the right seal bar design because they can have a critical influence, especially at seal initiation phase.},
  langid = {english},
  keywords = {contaminants at seal interface,flexible food packaging,heat seal strength,metallocene compounds,seal bar geometry},
  file = {/Users/amunozgo/Zotero/storage/T9GKCWHU/Ilhan et al. - 2021 - Effects of process parameters and solid particle contaminants on the seal strength of low-density po.pdf;/Users/amunozgo/Zotero/storage/7AKQR3K5/pts.html}
}

